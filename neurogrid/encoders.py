"""Generate encoders under various different constraints.

This module contains a bunch of different functions for generating
encoders.  Each function should have N and D as the first two
parameters (the number of encoders and their dimensionality), and
a numpy.random.RandomState object to act as a random number
generator as a third parameter.  Other case-specific parameters
can then follow.
"""

import numpy as np

def random(N, D, rng):
    """Create uniformly distributed encoders around the unit sphere.
    
    Encoders will all have a norm-2 of 1.0.
    
    :param integer N:   number of encoders to create
    :param integer D:   dimensionality of encoders
    :param numpy.random.RandomState rng:   random number generator
    """
    samples = rng.randn(N, D)
    norm = np.sqrt(np.sum(samples*samples, axis=1))
    return samples/norm[:,None]

def diamond(N, D, rng):
    """Create uniformly distributed encoders around the unit diamond.
    
    Encoders will all have a norm-1 of 1.0.  This is a bit weird,
    and is the only encoder function that produces encoders that
    are not norm-2=1.  The advantage of this is that every neuron
    will see the same total firing rate (excitatory + inhibitory).
    
    :param integer N:   number of encoders to create
    :param integer D:   dimensionality of encoders
    :param numpy.random.RandomState rng:   random number generator
    """
    samples = rng.randn(N, D)
    norm = np.sum(np.abs(samples), axis=1)
    return samples/norm[:,None]


def swapped(N, D, rng, rows, cols, iterations=100, encoders=None):
    """Swap encoders so that similar encoders are near each other.
    
    The algorithm is for each encoder in the initial set, randomly
    pick another encoder and check to see if swapping those two
    encoders would reduce the average difference between the
    encoders and their neighbours.  Difference is measured as the
    dot product.  Each encoder has four neighbours (N, S, E, W),
    except for the ones on the edges which have fewer (no wrapping).
    This algorithm is repeated `iterations` times, so a total of
    `iterations*N` swaps are considered.    
    
    :param integer N:   number of encoders to create
    :param integer D:   dimensionality of encoders
    :param numpy.random.RandomState rng:   random number generator
    :param integer rows:  number of rows (rows*cols must equal N)
    :param integer cols:  number of columns (rows*cols must equal N)
    :param integer iterations: number of times to repeat swapping
    :param encoders: initial set of encoders (if None, they are
                     generated by calling :func:`random()`)
    """
    
    assert rows*cols == N  # make sure the layout has a valid size

    if encoders is None:              # if we aren't given encoders
        encoders = random(N, D, rng)  #      make our own
        
    def score(encoders, index, rows, cols):
        """Helper function to compute similarity for one encoder.
        
        :param array encoders: the encoders
        :param integer index: the encoder to compute for
        :param integer rows: the width of the 2d grid
        :param integer cols: the height of the 2d grid
        """
        i = index % cols   # find the 2d location of the indexth element
        j = index / cols
        
        sim = 0     # total of dot products
        count = 0   # number of neighbours
        if i>0: # if we're not at the left edge, do the WEST comparison
            sim += np.dot(encoders[j*cols+i], encoders[j*cols+i-1])
            count += 1
        if i<cols-1:  # if we're not at the right edge, do EAST
            sim += np.dot(encoders[j*cols+i], encoders[j*cols+i+1])
            count += 1
        if j>0:   # if we're not at the top edge, do NORTH
            sim += np.dot(encoders[j*cols+i], encoders[(j-1)*cols+i])
            count += 1
        if j<rows-1:  # if we're not at the bottom edge, do SOUTH 
            sim += np.dot(encoders[j*cols+i], encoders[(j+1)*cols+i])
            count += 1
        return sim/count
        
    for k in range(iterations):
        target = rng.randint(0, N, N)  # pick random swap targets
        for i in range(N):
            j = target[i]
            if i != j:  # if not swapping with yourself
                # compute similarity score how we are (unswapped)
                sim1 = score(encoders, i, rows, cols) + score(encoders, 
                                                          j, rows, cols)
                # swap the encoder
                encoders[[i,j],:] = encoders[[j,i],:]
                # compute similarity score how we are (swapped)
                sim2 = score(encoders, i, rows, cols) + score(encoders, 
                                                          j, rows, cols)
                
                # if we were better unswapped
                if sim1 > sim2:
                    # swap them back
                    encoders[[i,j],:] = encoders[[j,i],:]
    
    return encoders            
            

def kohonen(N, D, rng, rows, cols, iterations=100, learning_rate=1.0, 
            N_samples=None):
    """Generate encoders using a Kohonen map
    
    Requires the mvpa2 library: http://www.pymvpa.org/.  The main
    function used is the SimpleSOMMapper: http://www.pymvpa.org/generated/mvpa2.mappers.som.SimpleSOMMapper.html
    
    The idea is to feed random data into the kohonen map and then use
    the learned weights in the map to be the set of encoders.
    
    :param integer N:   number of encoders to create
    :param integer D:   dimensionality of encoders
    :param numpy.random.RandomState rng:   random number generator
    :param integer rows:  number of rows (rows*cols must equal N)
    :param integer cols:  number of columns (rows*cols must equal N)
    :param integer iterations: number of training iterations
    :param float learning_rate: initial learning rate (will gradually
                                decrease during training).  The actual
                                used learning rate is `learning_rate/N`
                                as this seems to scale well.
    """
    assert rows * cols == N
    
    import mvpa2.suite
    
    # as we increase N, we need to decrease the learning rate to get
    # roughly similar behaviour.
    learning_rate = learning_rate/N
    
    # build the SOM (self-organizing map)
    som = mvpa2.suite.SimpleSOMMapper((rows, cols), iterations, learning_rate=learning_rate)

    # generate the samples and train the SOM
    if N_samples is None: N_samples = N
    samples = random(N_samples, D, rng)    
    som.train(samples)
    
    # read out the learned weights
    encoders = som.K
    encoders.shape = N,D
    
    return encoders
    
    
    
    
